{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this challenge, you will need to choose a corpus of data from nltk or another source that includes categories you can predict and create an analysis pipeline that includes the following steps:\n",
    "\n",
    "- Data cleaning / processing / language parsing\n",
    "- Create features using two different NLP methods: For example, BoW vs tf-idf.\n",
    "- Use the features to fit supervised learning models for each feature set to predict the category outcomes.\n",
    "- Assess your models using cross-validation and determine whether one model performed better.\n",
    "- Pick one of the models and try to increase accuracy by at least 5 percentage points.\n",
    "- Write up your report in a Jupyter notebook. Be sure to explicitly justify the choices you make throughout, and submit it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to C:\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CategorizedPlaintextCorpusReader in 'C:\\\\nltk_data\\\\corpora\\\\movie_reviews'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print (len(movie_reviews.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "print (movie_reviews.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print (len(movie_reviews.fileids('pos')))\n",
    "print (len(movie_reviews.fileids('neg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, we pick two files, a negative review and a positive review\n",
    "neg_rev_list = []\n",
    "for i in movie_reviews.fileids('neg')[0:5]:\n",
    "    #print(i)\n",
    "    neg_rev_list.append(movie_reviews.raw(i))\n",
    "all_neg_revs = ' '.join(neg_rev_list)\n",
    "\n",
    "pos_rev_list = []\n",
    "for i in movie_reviews.fileids('pos')[0:5]:\n",
    "    #print(i)\n",
    "    pos_rev_list.append(movie_reviews.raw(i))\n",
    "all_pos_revs = ' '.join(pos_rev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plot : two teen couples go to a church party , drink and then drive . \\nthey get into an accident . \\none of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \\nwhat\\'s the deal ? \\nwatch the movie and \" sorta \" find out . . . \\ncritique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \\nwhich is what makes this review an even harder one to write , since i generally applaud films which attempt'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_neg_revs[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before . \\nfor starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen . \\nto say moore and campbell thoroughly researched the subject\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pos_revs[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning(from the curriculum).\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    text = ' '.join(text.split())\n",
    "    text = re.sub(r' . . . ','. ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_rev_clean = text_cleaner(all_neg_revs)\n",
    "neg_rev_all_clean = neg_rev_clean.replace(\"\\\\\",'')\n",
    "\n",
    "pos_rev_clean = text_cleaner(all_pos_revs)\n",
    "pos_rev_all_clean = pos_rev_clean.replace(\"\\\\\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plot : two teen couples go to a church party , drink and then drive . \\nthey get into an accident . \\none of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \\nwhat\\'s the deal ? \\nwatch the movie and \" sorta \" find out . . . \\ncritique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \\nwhich is what makes this review an even harder one to write , since i generally applaud films which attempt'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_neg_revs[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plot : two teen couples go to a church party , drink and then drive . they get into an accident . one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . what\\'s the deal ? watch the movie and \" sorta \" find out. critique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . which is what makes this review an even harder one to write , since i generally applaud films which attempt to break t'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_rev_all_clean[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before . \\nfor starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen . \\nto say moore and campbell thoroughly researched the subject\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pos_revs[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before . for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen . to say moore and campbell thoroughly researched the subject o\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_rev_all_clean[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse using SpaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "neg_rev = nlp(neg_rev_all_clean)\n",
    "pos_rev = nlp(pos_rev_all_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(plot, :, two, teen, couples, go, to, a, churc...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(they, get, into, an, accident, .)</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(one, of, the, guys, dies, ,, but, his, girlfr...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(what, 's, the, deal, ?)</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(watch, the, movie, and, \", sorta, \", find, ou...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0         1\n",
       "0  (plot, :, two, teen, couples, go, to, a, churc...  Negative\n",
       "1                 (they, get, into, an, accident, .)  Negative\n",
       "2  (one, of, the, guys, dies, ,, but, his, girlfr...  Negative\n",
       "3                           (what, 's, the, deal, ?)  Negative\n",
       "4  (watch, the, movie, and, \", sorta, \", find, ou...  Negative"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Group into sentences\n",
    "neg_sents = [[sent,'Negative'] for sent in neg_rev.sents]\n",
    "pos_sents = [[sent, 'Positive'] for sent in pos_rev.sents]\n",
    "\n",
    "\n",
    "sentences_df = pd.DataFrame(neg_sents + pos_sents)\n",
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW function\n",
    "def bag_of_words(text, most_common_count):\n",
    "    \n",
    "    # filter out punctuation and stop words\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    print('allwords count', len(allwords))\n",
    "    # Return most common words\n",
    "    return [item[0] for item in Counter(allwords).most_common(most_common_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allwords count 1201\n",
      "allwords count 1710\n"
     ]
    }
   ],
   "source": [
    "# Get bags \n",
    "neg_words = bag_of_words(neg_rev, 500)\n",
    "\n",
    "pos_words = bag_of_words(pos_rev, 500)\n",
    "\n",
    "# Combine bags to create common set of unique words\n",
    "common_words = set(neg_words + pos_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bag of words data frame using combined common words and sentences\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Build data frame\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentences in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentences\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skip</th>\n",
       "      <th>police</th>\n",
       "      <th>deftly</th>\n",
       "      <th>horror</th>\n",
       "      <th>hunt</th>\n",
       "      <th>entire</th>\n",
       "      <th>twin</th>\n",
       "      <th>witch</th>\n",
       "      <th>design</th>\n",
       "      <th>palate</th>\n",
       "      <th>...</th>\n",
       "      <th>enter</th>\n",
       "      <th>scene</th>\n",
       "      <th>20th</th>\n",
       "      <th>high</th>\n",
       "      <th>carve</th>\n",
       "      <th>ape</th>\n",
       "      <th>hollow</th>\n",
       "      <th>indiglo</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(plot, :, two, teen, couples, go, to, a, churc...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(they, get, into, an, accident, .)</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(one, of, the, guys, dies, ,, but, his, girlfr...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(what, 's, the, deal, ?)</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(watch, the, movie, and, \", sorta, \", find, ou...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 889 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  skip police deftly horror hunt entire twin witch design palate  ... enter  \\\n",
       "0    0      0      0      0    0      0    0     0      0      0  ...     0   \n",
       "1    0      0      0      0    0      0    0     0      0      0  ...     0   \n",
       "2    0      0      0      0    0      0    0     0      0      0  ...     0   \n",
       "3    0      0      0      0    0      0    0     0      0      0  ...     0   \n",
       "4    0      0      0      0    0      0    0     0      0      0  ...     0   \n",
       "\n",
       "  scene 20th high carve ape hollow indiglo  \\\n",
       "0     0    0    0     0   0      0       0   \n",
       "1     0    0    0     0   0      0       0   \n",
       "2     0    0    0     0   0      0       0   \n",
       "3     0    0    0     0   0      0       0   \n",
       "4     0    0    0     0   0      0       0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (plot, :, two, teen, couples, go, to, a, churc...    Negative  \n",
       "1                 (they, get, into, an, accident, .)    Negative  \n",
       "2  (one, of, the, guys, dies, ,, but, his, girlfr...    Negative  \n",
       "3                           (what, 's, the, deal, ?)    Negative  \n",
       "4  (watch, the, movie, and, \", sorta, \", find, ou...    Negative  \n",
       "\n",
       "[5 rows x 889 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create bow features \n",
    "reviews = bow_features(sentences_df, common_words)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sents_list = []\n",
    "pos_sents_list = []\n",
    "all_sents_list = []\n",
    "for i in movie_reviews.fileids('neg')[0:5]:\n",
    "    \n",
    "    neg_rev_sents = movie_reviews.sents(i)\n",
    "    neg_sents_list.append([ \" \".join(sent) for sent in neg_rev_sents]    )\n",
    "\n",
    "for i in movie_reviews.fileids('pos')[0:5]:\n",
    "    \n",
    "    pos_rev_sents = movie_reviews.sents(i)\n",
    "    pos_sents_list.append([ \" \".join(sent) for sent in pos_rev_sents]    )\n",
    "\n",
    "neg_pos_sent = neg_sents_list + pos_sents_list\n",
    "\n",
    "for sublist in neg_pos_sent:\n",
    "    for item in sublist:\n",
    "        all_sents_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#vectorize\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, \n",
    "                             min_df=2, \n",
    "                             stop_words='english',   \n",
    "                             use_idf=True,\n",
    "                             norm=u'l2', \n",
    "                             smooth_idf=True \n",
    "                            )\n",
    "\n",
    "all_sents_tfidf = vectorizer.fit_transform(all_sents_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "161\n"
     ]
    }
   ],
   "source": [
    "neg_cnt = 0\n",
    "for sublist in neg_sents_list:\n",
    "    for item in sublist:\n",
    "        neg_cnt += 1\n",
    "print(neg_cnt)   \n",
    "pos_cnt = 0\n",
    "for sublist in pos_sents_list:\n",
    "    for item in sublist:\n",
    "        pos_cnt += 1\n",
    "print(pos_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model variables\n",
    "\n",
    "# BoW\n",
    "X_bow = reviews.drop(['text_sentence', 'text_source'], 1)\n",
    "Y_bow = reviews['text_source']\n",
    "\n",
    "# Tfidf\n",
    "X_tfidf = all_sents_tfidf\n",
    "Y_tfidf = ['Negative'] * neg_cnt + ['Positive'] * pos_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf,y_train_tfidf, y_test_tfidf= train_test_split(X_tfidf,Y_tfidf, test_size=0.3, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow, X_test_bow,y_train_bow, y_test_bow= train_test_split(X_bow,Y_bow, test_size=0.3, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: \n",
      " 0.7112068965517241\n",
      "Confusion Matrix: \n",
      " [[ 53  49]\n",
      " [ 18 112]]\n",
      "Cross Validation Score: \n",
      " [0.65957447 0.80851064 0.69565217 0.69565217 0.69565217]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.52      0.61       102\n",
      "    Positive       0.70      0.86      0.77       130\n",
      "\n",
      "    accuracy                           0.71       232\n",
      "   macro avg       0.72      0.69      0.69       232\n",
      "weighted avg       0.72      0.71      0.70       232\n",
      "\n",
      "Test Set Accuracy Score: \n",
      " 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#BoW\n",
    "log_r_bow = LogisticRegression(random_state=13)\n",
    "\n",
    "log_r_bow.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "y_train_bow_predlog = cross_val_predict(log_r_bow, X_train_bow, y_train_bow, cv=5)\n",
    "print('Accuracy Score: \\n' ,metrics.accuracy_score(y_train_bow, y_train_bow_predlog))\n",
    "print('Confusion Matrix: \\n' ,confusion_matrix(y_train_bow, y_train_bow_predlog))\n",
    "print('Cross Validation Score: \\n' ,cross_val_score(log_r_bow, X_train_bow, y_train_bow, cv=5, scoring='accuracy'))\n",
    "print('Classification Report: \\n' ,classification_report(y_train_bow, y_train_bow_predlog))\n",
    "\n",
    "y_test_bow_predlog = log_r_bow.predict(X_test_bow)\n",
    "print('Test Set Accuracy Score: \\n' ,metrics.accuracy_score(y_test_bow, y_test_bow_predlog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: \n",
      " 0.676056338028169\n",
      "Confusion Matrix: \n",
      " [[49 51]\n",
      " [18 95]]\n",
      "Cross Validation Score: \n",
      " [0.74418605 0.6744186  0.55813953 0.66666667 0.73809524]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.73      0.49      0.59       100\n",
      "    Positive       0.65      0.84      0.73       113\n",
      "\n",
      "    accuracy                           0.68       213\n",
      "   macro avg       0.69      0.67      0.66       213\n",
      "weighted avg       0.69      0.68      0.66       213\n",
      "\n",
      "Test Set Accuracy Score: \n",
      " 0.7717391304347826\n"
     ]
    }
   ],
   "source": [
    "#tfidf\n",
    "log_r_tfidf = LogisticRegression(random_state=13)\n",
    "\n",
    "log_r_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "y_train_tfidf_predlog = cross_val_predict(log_r_tfidf, X_train_tfidf, y_train_tfidf, cv=5)\n",
    "print('Accuracy Score: \\n' ,metrics.accuracy_score(y_train_tfidf, y_train_tfidf_predlog))\n",
    "print('Confusion Matrix: \\n' ,confusion_matrix(y_train_tfidf, y_train_tfidf_predlog))\n",
    "print('Cross Validation Score: \\n' ,cross_val_score(log_r_tfidf, X_train_tfidf, y_train_tfidf, cv=5, scoring='accuracy'))\n",
    "print('Classification Report: \\n' ,classification_report(y_train_tfidf, y_train_tfidf_predlog))\n",
    "\n",
    "y_test_tfidf_predlog = log_r_tfidf.predict(X_test_tfidf)\n",
    "print('Test Set Accuracy Score: \\n' ,metrics.accuracy_score(y_test_tfidf, y_test_tfidf_predlog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: \n",
      " 0.6637931034482759\n",
      "Confusion Matrix: \n",
      " [[ 33  69]\n",
      " [  9 121]]\n",
      "Cross Validation Score: \n",
      " [0.63829787 0.68085106 0.65217391 0.7173913  0.63043478]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.79      0.32      0.46       102\n",
      "    Positive       0.64      0.93      0.76       130\n",
      "\n",
      "    accuracy                           0.66       232\n",
      "   macro avg       0.71      0.63      0.61       232\n",
      "weighted avg       0.70      0.66      0.63       232\n",
      "\n",
      "Test Set Accuracy Score: \n",
      " 0.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "#BoW\n",
    "rfc_bow = ensemble.RandomForestClassifier(random_state=13)\n",
    "\n",
    "rfc_bow.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "y_train_bow_predrfc = cross_val_predict(rfc_bow, X_train_bow, y_train_bow, cv=5)\n",
    "print('Accuracy Score: \\n' ,metrics.accuracy_score(y_train_bow, y_train_bow_predrfc))\n",
    "print('Confusion Matrix: \\n' ,confusion_matrix(y_train_bow, y_train_bow_predrfc))\n",
    "print('Cross Validation Score: \\n' ,cross_val_score(rfc_bow, X_train_bow, y_train_bow, cv=5, scoring='accuracy'))\n",
    "print('Classification Report: \\n' ,classification_report(y_train_bow, y_train_bow_predrfc))\n",
    "\n",
    "y_test_bow_predrfc = rfc_bow.predict(X_test_bow)\n",
    "print('Test Set Accuracy Score: \\n' ,metrics.accuracy_score(y_test_bow, y_test_bow_predrfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: \n",
      " 0.6995305164319249\n",
      "Confusion Matrix: \n",
      " [[85 15]\n",
      " [49 64]]\n",
      "Cross Validation Score: \n",
      " [0.69767442 0.74418605 0.62790698 0.71428571 0.71428571]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.63      0.85      0.73       100\n",
      "    Positive       0.81      0.57      0.67       113\n",
      "\n",
      "    accuracy                           0.70       213\n",
      "   macro avg       0.72      0.71      0.70       213\n",
      "weighted avg       0.73      0.70      0.69       213\n",
      "\n",
      "Test Set Accuracy Score: \n",
      " 0.7282608695652174\n"
     ]
    }
   ],
   "source": [
    "#tfidf\n",
    "rfc_tfidf = ensemble.RandomForestClassifier(random_state=13)\n",
    "\n",
    "rfc_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "y_train_tfidf_predrfc = cross_val_predict(rfc_tfidf, X_train_tfidf, y_train_tfidf, cv=5)\n",
    "print('Accuracy Score: \\n' ,metrics.accuracy_score(y_train_tfidf, y_train_tfidf_predrfc))\n",
    "print('Confusion Matrix: \\n' ,confusion_matrix(y_train_tfidf, y_train_tfidf_predrfc))\n",
    "print('Cross Validation Score: \\n' ,cross_val_score(rfc_tfidf, X_train_tfidf, y_train_tfidf, cv=5, scoring='accuracy'))\n",
    "print('Classification Report: \\n' ,classification_report(y_train_tfidf, y_train_tfidf_predrfc))\n",
    "\n",
    "y_test_tfidf_predrfc = rfc_tfidf.predict(X_test_tfidf)\n",
    "print('Test Set Accuracy Score: \\n' ,metrics.accuracy_score(y_test_tfidf, y_test_tfidf_predrfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: \n",
      " 0.6422413793103449\n",
      "Confusion Matrix: \n",
      " [[ 20  82]\n",
      " [  1 129]]\n",
      "Cross Validation Score: \n",
      " [0.61702128 0.68085106 0.63043478 0.67391304 0.60869565]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.95      0.20      0.33       102\n",
      "    Positive       0.61      0.99      0.76       130\n",
      "\n",
      "    accuracy                           0.64       232\n",
      "   macro avg       0.78      0.59      0.54       232\n",
      "weighted avg       0.76      0.64      0.57       232\n",
      "\n",
      "Test Set Accuracy Score: \n",
      " 0.62\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#BoW\n",
    "svc_bow = SVC(random_state=13)\n",
    "\n",
    "svc_bow.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "y_train_bow_predsvc = cross_val_predict(svc_bow, X_train_bow, y_train_bow, cv=5)\n",
    "print('Accuracy Score: \\n' ,metrics.accuracy_score(y_train_bow, y_train_bow_predsvc))\n",
    "print('Confusion Matrix: \\n' ,confusion_matrix(y_train_bow, y_train_bow_predsvc))\n",
    "print('Cross Validation Score: \\n' ,cross_val_score(svc_bow, X_train_bow, y_train_bow, cv=5, scoring='accuracy'))\n",
    "print('Classification Report: \\n' ,classification_report(y_train_bow, y_train_bow_predsvc))\n",
    "\n",
    "y_test_bow_predsvc = svc_bow.predict(X_test_bow)\n",
    "print('Test Set Accuracy Score: \\n' ,metrics.accuracy_score(y_test_bow, y_test_bow_predsvc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: \n",
      " 0.6666666666666666\n",
      "Confusion Matrix: \n",
      " [[45 55]\n",
      " [16 97]]\n",
      "Cross Validation Score: \n",
      " [0.55813953 0.74418605 0.58139535 0.76190476 0.69047619]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.74      0.45      0.56       100\n",
      "    Positive       0.64      0.86      0.73       113\n",
      "\n",
      "    accuracy                           0.67       213\n",
      "   macro avg       0.69      0.65      0.65       213\n",
      "weighted avg       0.68      0.67      0.65       213\n",
      "\n",
      "Test Set Accuracy Score: \n",
      " 0.717391304347826\n"
     ]
    }
   ],
   "source": [
    "#tfidf\n",
    "svc_tfidf = SVC(random_state=13)\n",
    "svc_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "y_train_tfidf_predsvc = cross_val_predict(svc_tfidf, X_train_tfidf, y_train_tfidf, cv=5)\n",
    "print('Accuracy Score: \\n' ,metrics.accuracy_score(y_train_tfidf, y_train_tfidf_predsvc))\n",
    "print('Confusion Matrix: \\n' ,confusion_matrix(y_train_tfidf, y_train_tfidf_predsvc))\n",
    "print('Cross Validation Score: \\n' ,cross_val_score(svc_tfidf, X_train_tfidf, y_train_tfidf, cv=5, scoring='accuracy'))\n",
    "print('Classification Report: \\n' ,classification_report(y_train_tfidf, y_train_tfidf_predsvc))\n",
    "\n",
    "y_test_tfidf_predsvc = svc_tfidf.predict(X_test_tfidf)\n",
    "print('Test Set Accuracy Score: \\n' ,metrics.accuracy_score(y_test_tfidf, y_test_tfidf_predsvc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allwords count 1201\n",
      "allwords count 1710\n"
     ]
    }
   ],
   "source": [
    "# Increase the sample size \n",
    "neg_words = bag_of_words(neg_rev, 500)\n",
    "\n",
    "pos_words = bag_of_words(pos_rev, 500)\n",
    "\n",
    "# Combine bags to create common set of unique words\n",
    "common_words = set(neg_words + pos_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skip</th>\n",
       "      <th>police</th>\n",
       "      <th>deftly</th>\n",
       "      <th>horror</th>\n",
       "      <th>hunt</th>\n",
       "      <th>entire</th>\n",
       "      <th>twin</th>\n",
       "      <th>witch</th>\n",
       "      <th>design</th>\n",
       "      <th>palate</th>\n",
       "      <th>...</th>\n",
       "      <th>enter</th>\n",
       "      <th>scene</th>\n",
       "      <th>20th</th>\n",
       "      <th>high</th>\n",
       "      <th>carve</th>\n",
       "      <th>ape</th>\n",
       "      <th>hollow</th>\n",
       "      <th>indiglo</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(plot, :, two, teen, couples, go, to, a, churc...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(they, get, into, an, accident, .)</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(one, of, the, guys, dies, ,, but, his, girlfr...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(what, 's, the, deal, ?)</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(watch, the, movie, and, \", sorta, \", find, ou...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 889 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  skip police deftly horror hunt entire twin witch design palate  ... enter  \\\n",
       "0    0      0      0      0    0      0    0     0      0      0  ...     0   \n",
       "1    0      0      0      0    0      0    0     0      0      0  ...     0   \n",
       "2    0      0      0      0    0      0    0     0      0      0  ...     0   \n",
       "3    0      0      0      0    0      0    0     0      0      0  ...     0   \n",
       "4    0      0      0      0    0      0    0     0      0      0  ...     0   \n",
       "\n",
       "  scene 20th high carve ape hollow indiglo  \\\n",
       "0     0    0    0     0   0      0       0   \n",
       "1     0    0    0     0   0      0       0   \n",
       "2     0    0    0     0   0      0       0   \n",
       "3     0    0    0     0   0      0       0   \n",
       "4     0    0    0     0   0      0       0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (plot, :, two, teen, couples, go, to, a, churc...    Negative  \n",
       "1                 (they, get, into, an, accident, .)    Negative  \n",
       "2  (one, of, the, guys, dies, ,, but, his, girlfr...    Negative  \n",
       "3                           (what, 's, the, deal, ?)    Negative  \n",
       "4  (watch, the, movie, and, \", sorta, \", find, ou...    Negative  \n",
       "\n",
       "[5 rows x 889 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create bow features \n",
    "reviews = bow_features(sentences_df, common_words)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_types(df):\n",
    "    \n",
    "    person_ent_type = []\n",
    "    qty_ent_type = []\n",
    "    ordinal_ent_type = []\n",
    "    time_ent_type = []\n",
    "    org_ent_type = []\n",
    "    lang_ent_type = []\n",
    "    date_ent_type = []\n",
    "    card_ent_type = []\n",
    "    gpe_ent_type = []\n",
    "    fac_ent_type = []\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        person_count = 0\n",
    "        qty_count= 0\n",
    "        ordinal_count = 0\n",
    "        time_count = 0\n",
    "        org_count = 0\n",
    "        lang_count = 0\n",
    "        date_count= 0\n",
    "        cardinal_count =0 \n",
    "        gpe_count= 0\n",
    "        fac_count = 0\n",
    "    \n",
    "        for token in sentence:\n",
    "            if token.ent_type_ == 'PERSON':\n",
    "                person_count += 1\n",
    "        \n",
    "            if token.ent_type_ == 'QUANTITY':\n",
    "                qty_count += 1\n",
    "            \n",
    "            if token.ent_type_ == 'ORDINAL':\n",
    "                ordinal_count += 1\n",
    "            \n",
    "            if token.ent_type_ == 'TIME':\n",
    "                time_count += 1\n",
    "            \n",
    "            if token.ent_type_ == 'ORG':\n",
    "                org_count += 1\n",
    "            \n",
    "            if token.ent_type_ == 'LANGUAGE':\n",
    "                lang_count += 1\n",
    "            if token.ent_type_ == 'DATE':\n",
    "                date_count += 1            \n",
    "        \n",
    "            if token.ent_type_ == 'CARDINAL':\n",
    "                cardinal_count += 1            \n",
    "            if token.ent_type_ == 'GPE':\n",
    "                gpe_count += 1            \n",
    "            if token.ent_type_ == 'FAC':\n",
    "                fac_count += 1            \n",
    "            \n",
    "        person_ent_type.append(person_count)\n",
    "        qty_ent_type.append(qty_count)\n",
    "        ordinal_ent_type.append(ordinal_count)\n",
    "        time_ent_type.append(time_count)\n",
    "        org_ent_type.append(org_count)\n",
    "        lang_ent_type.append(lang_count)\n",
    "        date_ent_type.append(date_count)\n",
    "        card_ent_type.append(cardinal_count)\n",
    "        gpe_ent_type.append(gpe_count)\n",
    "        fac_ent_type.append(fac_count)\n",
    "\n",
    "          \n",
    "    df['person_ent'] = person_ent_type\n",
    "    df['qty_ent'] = qty_ent_type\n",
    "    df['ordinal_ent'] = ordinal_ent_type\n",
    "    df['time_ent'] = time_ent_type\n",
    "    df['org_ent'] = org_ent_type\n",
    "    df['lang_ent'] = lang_ent_type\n",
    "    df['date_ent'] = date_ent_type\n",
    "    df['card_ent'] = card_ent_type\n",
    "    df['gpe_ent'] = gpe_ent_type\n",
    "    df['fac_ent'] = fac_ent_type\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = entity_types(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skip</th>\n",
       "      <th>police</th>\n",
       "      <th>deftly</th>\n",
       "      <th>horror</th>\n",
       "      <th>hunt</th>\n",
       "      <th>entire</th>\n",
       "      <th>twin</th>\n",
       "      <th>witch</th>\n",
       "      <th>design</th>\n",
       "      <th>palate</th>\n",
       "      <th>...</th>\n",
       "      <th>person_ent</th>\n",
       "      <th>qty_ent</th>\n",
       "      <th>ordinal_ent</th>\n",
       "      <th>time_ent</th>\n",
       "      <th>org_ent</th>\n",
       "      <th>lang_ent</th>\n",
       "      <th>date_ent</th>\n",
       "      <th>card_ent</th>\n",
       "      <th>gpe_ent</th>\n",
       "      <th>fac_ent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 899 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  skip police deftly horror hunt entire twin witch design palate  ...  \\\n",
       "0    0      0      0      0    0      0    0     0      0      0  ...   \n",
       "1    0      0      0      0    0      0    0     0      0      0  ...   \n",
       "2    0      0      0      0    0      0    0     0      0      0  ...   \n",
       "3    0      0      0      0    0      0    0     0      0      0  ...   \n",
       "4    0      0      0      0    0      0    0     0      0      0  ...   \n",
       "\n",
       "  person_ent qty_ent ordinal_ent time_ent org_ent lang_ent date_ent card_ent  \\\n",
       "0          0       0           0        0       0        0        0        1   \n",
       "1          0       0           0        0       0        0        0        0   \n",
       "2          0       0           0        0       0        0        0        1   \n",
       "3          0       0           0        0       0        0        0        0   \n",
       "4          0       0           0        0       0        0        0        0   \n",
       "\n",
       "  gpe_ent fac_ent  \n",
       "0       0       0  \n",
       "1       0       0  \n",
       "2       0       0  \n",
       "3       0       0  \n",
       "4       0       0  \n",
       "\n",
       "[5 rows x 899 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow = reviews.drop(['text_sentence', 'text_source'], 1)\n",
    "Y_bow = reviews['text_source']\n",
    "X_train_bow, X_test_bow,y_train_bow, y_test_bow= train_test_split(X_bow, Y_bow, test_size=0.3, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: \n",
      " 0.7025862068965517\n",
      "Confusion Matrix: \n",
      " [[ 53  49]\n",
      " [ 20 110]]\n",
      "Cross Validation Score: \n",
      " [0.63829787 0.74468085 0.73913043 0.76086957 0.63043478]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.73      0.52      0.61       102\n",
      "    Positive       0.69      0.85      0.76       130\n",
      "\n",
      "    accuracy                           0.70       232\n",
      "   macro avg       0.71      0.68      0.68       232\n",
      "weighted avg       0.71      0.70      0.69       232\n",
      "\n",
      "Test Set Accuracy Score: \n",
      " 0.74\n"
     ]
    }
   ],
   "source": [
    "#BoW\n",
    "log_r_bow = LogisticRegression(random_state=13)\n",
    "\n",
    "log_r_bow.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "y_train_bow_predlog = cross_val_predict(log_r_bow, X_train_bow, y_train_bow, cv=5)\n",
    "print('Accuracy Score: \\n' ,metrics.accuracy_score(y_train_bow, y_train_bow_predlog))\n",
    "print('Confusion Matrix: \\n' ,confusion_matrix(y_train_bow, y_train_bow_predlog))\n",
    "print('Cross Validation Score: \\n' ,cross_val_score(log_r_bow, X_train_bow, y_train_bow, cv=5, scoring='accuracy'))\n",
    "print('Classification Report: \\n' ,classification_report(y_train_bow, y_train_bow_predlog))\n",
    "\n",
    "y_test_bow_predlog = log_r_bow.predict(X_test_bow)\n",
    "print('Test Set Accuracy Score: \\n' ,metrics.accuracy_score(y_test_bow, y_test_bow_predlog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter  {'C': 1, 'fit_intercept': True, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "#tune the parameters\n",
    "parameters =[ {'C': [0.01, 0.1, 1, 10, 100],'solver':['liblinear'],'penalty':['l1', 'l2'],'fit_intercept':[True]},\n",
    "            {'C': [0.01, 0.1, 1, 10, 100],'solver':['lbfgs','newton-cg'],'fit_intercept':[True]}\n",
    "            ]\n",
    "\n",
    "grid_logr = GridSearchCV(log_r_bow, param_grid = parameters )\n",
    "grid_logr.fit(X_train_bow, y_train_bow)\n",
    "print('Best Parameter ', grid_logr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: \n",
      " 0.7025862068965517\n",
      "Confusion Matrix: \n",
      " [[ 53  49]\n",
      " [ 20 110]]\n",
      "Cross Validation Score: \n",
      " [0.63829787 0.74468085 0.73913043 0.76086957 0.63043478]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.73      0.52      0.61       102\n",
      "    Positive       0.69      0.85      0.76       130\n",
      "\n",
      "    accuracy                           0.70       232\n",
      "   macro avg       0.71      0.68      0.68       232\n",
      "weighted avg       0.71      0.70      0.69       232\n",
      "\n",
      "Test Set Accuracy Score: \n",
      " 0.74\n"
     ]
    }
   ],
   "source": [
    "log_r_tuned = LogisticRegression(**grid_logr.best_params_, random_state = 13)\n",
    "log_r_tuned.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "y_train_bow_predlog = cross_val_predict(log_r_tuned, X_train_bow, y_train_bow, cv=5)\n",
    "print('Accuracy Score: \\n' ,metrics.accuracy_score(y_train_bow, y_train_bow_predlog))\n",
    "print('Confusion Matrix: \\n' ,confusion_matrix(y_train_bow, y_train_bow_predlog))\n",
    "print('Cross Validation Score: \\n' ,cross_val_score(log_r_tuned, X_train_bow, y_train_bow, cv=5, scoring='accuracy'))\n",
    "print('Classification Report: \\n' ,classification_report(y_train_bow, y_train_bow_predlog))\n",
    "\n",
    "y_test_bow_predlog = log_r_tuned.predict(X_test_bow)\n",
    "print('Test Set Accuracy Score: \\n' ,metrics.accuracy_score(y_test_bow, y_test_bow_predlog))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding entity features to the model seems to decrease the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
